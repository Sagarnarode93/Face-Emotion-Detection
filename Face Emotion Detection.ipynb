{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f215343978ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# Resize the video images by (1280,720) as a laptop screen size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1280\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m720\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Importing some basic libraries\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import model_from_json\n",
    "\n",
    "#There are 7 classes present in the data set\n",
    "\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('model/emotion_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "emotion_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "emotion_model.load_weights(\"model/emotion_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# start the webcam \n",
    "\n",
    "#cap = cv2.VideoCapture(0)\n",
    "\n",
    "# pass here video path\n",
    "\n",
    "cap = cv2.VideoCapture(\"C:\\\\Users\\\\Lenovo\\\\Desktop\\\\Deep learning\\\\Emotion detection\\\\videos\\\\emotion_sample3.mp4\")\n",
    "\n",
    "while True:\n",
    "    # Resize the video images by (1280,720) as a laptop screen size\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, (1280, 720))\n",
    "    if not ret:\n",
    "        break            #Detect face before emotion detection using haarcascade file\n",
    "    face_detector = cv2.CascadeClassifier('C:\\\\Users\\\\Lenovo\\\\Desktop\\\\Deep learning\\\\Emotion detection\\\\haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # detect faces available on camera\n",
    "    num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    # take each face available on the camera and Preprocess it\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "\n",
    "        # predict the emotions\n",
    "        emotion_prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'streamlit_webrtc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-95788f0c88b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstreamlit_webrtc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwebrtc_streamer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVideoTransformerBase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# load model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'streamlit_webrtc'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import streamlit as st\n",
    "from tensorflow import keras\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from streamlit_webrtc import webrtc_streamer, VideoTransformerBase\n",
    "\n",
    "# load model\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "# load json and create model\n",
    "json_file = open('model/emotion_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "classifier = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "classifier.load_weights(\"model/emotion_model.h5\")\n",
    "\n",
    "#load face\n",
    "try:\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "except Exception:\n",
    "    st.write(\"Error loading cascade classifiers\")\n",
    "\n",
    "class VideoTransformer(VideoTransformerBase):\n",
    "    def transform(self, frame):\n",
    "        img = frame.to_ndarray(format=\"bgr24\")\n",
    "\n",
    "        #image gray\n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            image=img_gray, scaleFactor=1.3, minNeighbors=5)\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(img=img, pt1=(x, y), pt2=(\n",
    "                x + w, y + h), color=(255, 0, 0), thickness=2)\n",
    "            roi_gray = img_gray[y:y + h, x:x + w]\n",
    "            roi_gray = cv2.resize(roi_gray, (48, 48), interpolation=cv2.INTER_AREA)\n",
    "            if np.sum([roi_gray]) != 0:\n",
    "                roi = roi_gray.astype('float') / 255.0\n",
    "                roi = img_to_array(roi)\n",
    "                roi = np.expand_dims(roi, axis=0)\n",
    "                prediction = classifier.predict(roi)[0]\n",
    "                maxindex = int(np.argmax(prediction))\n",
    "                finalout = emotion_dict[maxindex]\n",
    "                output = str(finalout)\n",
    "            label_position = (x, y)\n",
    "            cv2.putText(img, output, label_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        return img\n",
    "\n",
    "def main():\n",
    "    # Face Analysis Application #\n",
    "    st.title(\"Real Time Face Emotion Detection Application\")\n",
    "    activiteis = [\"Home\", \"Webcam Face Detection\", \"About\"]\n",
    "    choice = st.sidebar.selectbox(\"Select Activity\", activiteis)\n",
    "    st.sidebar.markdown(\n",
    "        \"\"\" Developed by Mohammad Juned Khan    \n",
    "            Email : Mohammad.juned.z.khan@gmail.com  \n",
    "            [LinkedIn] (https://www.linkedin.com/in/md-juned-khan)\"\"\")\n",
    "    if choice == \"Home\":\n",
    "        html_temp_home1 = \"\"\"<div style=\"background-color:#6D7B8D;padding:10px\">\n",
    "                                            <h4 style=\"color:white;text-align:center;\">\n",
    "                                            Face Emotion detection application using OpenCV, Custom CNN model and Streamlit.</h4>\n",
    "                                            </div>\n",
    "                                            </br>\"\"\"\n",
    "        st.markdown(html_temp_home1, unsafe_allow_html=True)\n",
    "        st.write(\"\"\"\n",
    "                 The application has two functionalities.\n",
    "\n",
    "                 1. Real time face detection using web cam feed.\n",
    "\n",
    "                 2. Real time face emotion recognization.\n",
    "\n",
    "                 \"\"\")\n",
    "    elif choice == \"Webcam Face Detection\":\n",
    "        st.header(\"Webcam Live Feed\")\n",
    "        st.write(\"Click on start to use webcam and detect your face emotion\")\n",
    "        webrtc_streamer(key=\"example\", video_transformer_factory=VideoTransformer)\n",
    "\n",
    "    elif choice == \"About\":\n",
    "        st.subheader(\"About this app\")\n",
    "        html_temp_about1= \"\"\"<div style=\"background-color:#6D7B8D;padding:10px\">\n",
    "                                    <h4 style=\"color:white;text-align:center;\">\n",
    "                                    Real time face emotion detection application using OpenCV, Custom Trained CNN model and Streamlit.</h4>\n",
    "                                    </div>\n",
    "                                    </br>\"\"\"\n",
    "        st.markdown(html_temp_about1, unsafe_allow_html=True)\n",
    "\n",
    "        html_temp4 = \"\"\"\n",
    "                             <div style=\"background-color:#98AFC7;padding:10px\">\n",
    "                             <h4 style=\"color:white;text-align:center;\">This Application is developed by Mohammad Juned Khan using Streamlit Framework, Opencv, Tensorflow and Keras library for demonstration purpose. If you're on LinkedIn and want to connect, just click on the link in sidebar and shoot me a request. If you have any suggestion or wnat to comment just write a mail at Mohammad.juned.z.khan@gmail.com. </h4>\n",
    "                             <h4 style=\"color:white;text-align:center;\">Thanks for Visiting</h4>\n",
    "                             </div>\n",
    "                             <br></br>\n",
    "                             <br></br>\"\"\"\n",
    "\n",
    "        st.markdown(html_temp4, unsafe_allow_html=True)\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "! pip install streamlit-webrtc opencv-python-headless matplotlib pydub\n",
    "! streamlit run https://raw.githubusercontent.com/whitphx/streamlit-webrtc-example/main/app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
